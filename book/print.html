<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CS394 Lectures</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="lecture_1.html"><strong aria-hidden="true">1.</strong> Lecture 1</a></li><li class="chapter-item expanded "><a href="lecture_2.html"><strong aria-hidden="true">2.</strong> Lecture 2</a></li><li class="chapter-item expanded "><a href="lecture_3.html"><strong aria-hidden="true">3.</strong> Lecture 3</a></li><li class="chapter-item expanded "><a href="lecture_4.html"><strong aria-hidden="true">4.</strong> Lecture 4</a></li><li class="chapter-item expanded "><a href="lecture_5.html"><strong aria-hidden="true">5.</strong> Lecture 5</a></li><li class="chapter-item expanded "><a href="lecture_6.html"><strong aria-hidden="true">6.</strong> Lecture 6</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">CS394 Lectures</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" name="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="lecture-1"><a class="header" href="#lecture-1">Lecture 1</a></h1>
<p>(didn't take notes for this one yet!)</p>
<h1 id="lecture-2"><a class="header" href="#lecture-2">Lecture 2</a></h1>
<div class="def">
<p><strong>The Counting Principle</strong>: <em>Consider a process of \(r\) stages. Suppose that:</em></p>
<ol>
<li>There are \(n_1\) possible results at the first stage.</li>
<li>For every possible result at the first stage, there are \(n_2\) possible results at the second stage.</li>
<li>More generally, for any sequence of possible results at the first \(i - 1\) stages, there are \(n_1\) possible results at the \(i^{th}\) stage.</li>
</ol>
<ul>
<li>Then, the total number of possible results of the r-stage process is
\[
n_1n_2 \cdots n_r
\]</li>
</ul>
</div>
<div class="idea">
<p><strong>Big Idea</strong>: <em>For problems <strong>without permutation or replacement</strong>, a series of decisions can be enumerated by multiplying the number of possible decisions at each stage.</em></p>
</div>
<h2 id="extra-credit-favorite-sport-football"><a class="header" href="#extra-credit-favorite-sport-football"><strong>Extra Credit</strong>: Favorite Sport: Football</a></h2>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> A pixel in a digital image is represented by 8 bits. How many encodings are possible for one pixel?
</summary>
<p>\[
2^8 = 256
\]</p>
</details>
</div>
<div class="def">
<p><strong>Mutually Exclusive Counting</strong>: <em>Suppose that \(X_1, \dots, X_t\) are sets and that the \(i^{th}\) set \(X_i\) has \(n_i\) elements. If \({X_1 \dots, X_i}\) is a <em>pairwise disjoint</em> family, the number of possible elements that can be selected from \(X_1\) or \(X_2\) or \( \dots \) or \(X_t\) is</em>
\[
n_1 + n_2 + \dots + n_t
\]</p>
</div>
<div class="idea">
<p><strong>Big Idea</strong>: <em>Because the set family is pairwise disjoint, we can think of these sets like bubbles on a Venn Diagram that don't overlap. Counting total elements across these sets is the same as just adding the number of elements from each.</em></p>
</div>
<div class="ex">
<details>
<summary>
<strong>Example</strong>: A six person committee Alice, Ben, Connie, Dolph, Egbert, and Francisco is to select a chairperson, secretary, and treasurer.
</summary>
<ol>
<li>In how many ways can this be done?
\[
6\cdot5\cdot4=120
\]</li>
<li>In how many ways can this be done if Alice or Ben must be chairman?
\[
5\cdot4 + 5\cdot 4 = 40
\]</li>
<li>How many ways if Egbert holds one of the offices?
\[
\begin{align}
5\cdot4 + 5\cdot4 + 5\cdot4 &amp;= 60\\
3(5\cdot4) &amp;= 60
\end{align}
\]</li>
<li>How many ways if both Dolph and Francisco holds one of the offices?
\[
4\cdot3\cdot2=24
\]</li>
</ol>
</details>
</div>
<div class="def">
<p><strong>Inclusion-Exclusion Principle for Two Sets</strong>:
\[
|X \cup Y| = |X| + |Y| - |X \cap Y|
\]</p>
</div>
<div class="idea">
<p><strong>Big Idea</strong>: <em>If two sets intersect, adding the cardinality of both individually would double-count the overlap so it's subtracted away.</em></p>
</div>
<div class="def">
<p><strong>Permutation</strong>: <em>A permutation of \(n\) distinct elements \(x_1, \dots, x_n\) is an ordering of the \(n\) elements \(x_1, \dots, x_n\). There are \(n!\) permutations of \(n\) elements.</em></p>
</div>
<div class="ex">
<details>
<summary><strong>Example</strong>: ABCDEF Permutations</summary>
<ol>
<li>How many permutations of the letters ABCDEF are there with the substring DEF?
<ul>
<li><em>Consider DEF to be a single element:</em>
\[
4! = 24
\]</li>
</ul>
</li>
<li>How many permutations of the letters ABCDEF are there with DEF together in any order?
<ul>
<li><em>Number of ways to order DEF is \(3!\). The number of permutations with DEF together is</em>
\[
3!4! = 144
\]</li>
</ul>
</li>
<li>How many ways can you seat 6 people around a round table? (if everybody moves \(n\) seats to the left or right, that is considered the same seating)
\[
5!
\]</li>
</ol>
</details>
</div>
<div class="def">
<p><strong>R-Permutation</strong>: <em>An r-permutation of \(n\) distinct elements \(x_1, \dots, x_n\) is an ordering of an r-element subset of \(x_1, \dots, x_n\). The number of r-permutations of n distinct elements is</em>
\[
P(n, r) = \frac{n!}{(n-r)!}
\]</p>
</div>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> How many ways can we select a chairperson, vice-chairperson, secretary, and treasurer froma  group of 10 persons?
</summary>
<p>\[
P(10, 4) = 10 \cdot 9 \cdot 8 \cdot 7
\]</p>
</details>
</div>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> In how many ways can seven distinct Martians and five distinct Neptunians wait in line if no two Neptunians stand together?
</summary>
<ul>
<li>The Martians can stand together however they'd like, so there are \(7!\) options for them. Because the Neptunians must have a Martian between them, there are 8 possible positions for them to stand. This means there are \(P(8, 5)\) possible orderings of Neptunians.</li>
</ul>
<p>\[
7! \cdot P(8, 5) = 33,868,800
\]</p>
</details>
</div>
<div class="def">
<p><strong>Combination</strong>: <em>An r-combination of \(X\) is an unordered selection of r elements of \(X\). The number of r-combinations of \(n\) distinct objects is</em>
\[
C(n, r) = \frac{P(n, r)}{r!} = \frac{n!}{(n-r)!r!}
\]</p>
<ul>
<li><strong>Notation</strong>: \(C(n, r) = \binom{n}{r} \)</li>
</ul>
</div>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> In how many ways can we select a committee of two women and three men from a group of 5 distinct women and 6 distinct men?
</summary>
<p>\[
C(5, 2) \cdot C(6, 3) = 200
\]</p>
</details>
</div>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> How many 8-bit strings contain exactly 4 ones?
</summary>
<p>\[
C(8, 4) = 70
\]</p>
</details>
</div>
<h1 id="lecture-3"><a class="header" href="#lecture-3">Lecture 3</a></h1>
<h3 id="sample-space--events"><a class="header" href="#sample-space--events">Sample Space &amp; Events</a></h3>
<p>Consider an experiment whose outcome is not predictable with certainty. The set of all possible outcomes of an experiment is known as the <em>sample space</em> of the experiment and is denoted by \(\Omega\).</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> What is the sample space if the outcome of an experiment consists of the determination of the sex of a newborn child? What about the order of finish in a race among 7 horses numbered 1-7?
</summary>
<p>\[
\Omega = \{ g, b \}\\
\Omega = \{ \text{All } 7! \text{ permutations of } (1, 2, 3, 4, 5, 6, 7) \}
\]</p>
</details>
</div>
<p>Any subset \(A\) of the sample space is known as an event.</p>
<p>The event \(A \cup B\) occurs if either \(A\) or \(B\) occurs.</p>
<h3 id="extra-credit-the-cake-is-a-lie"><a class="header" href="#extra-credit-the-cake-is-a-lie">Extra Credit: The Cake is a Lie</a></h3>
<p>For any two events \(A\) and \(B\), we define the event \(AB\) as the <em>intersection</em> of \(A\) and \(B\), or \(A \cap B\).</p>
<p>The event consisting of no outcomes is denoted as \(\emptyset\).</p>
<p>If \(AB = \emptyset\), the \(A\) and \(B\) are said to be mutually exclusive.</p>
<p>We define the event which consists of all outcomes in \(A_n\) for at least one value of \(n = 1, 2, \dots\) as:
\[
A = \bigcup_{n=1}^\infty A_n
\]</p>
<p>We define the event which consists of those outcomes in all events \(A_n\) for \(n=1, 2, \dots\) as:
\[
A = \bigcap_{n=1}^\infty A_n
\]</p>
<p>We define \(A^c\) as the <em>complement</em> of \(A\). These are all events in \(\Omega\) but not in \(A\).</p>
<p>If all events in \(A\) are also in \(B\), then we can write \(A \subset B\).</p>
<p>If \(A\) contains exactly the same events as \(B\), then \(A = B\).</p>
<h3 id="set-operations"><a class="header" href="#set-operations">Set Operations</a></h3>
<p>\[ \left(\bigcup_{i=1}^n A_i\right)^c = \bigcap_{i=1}^n A_i^c \]
In other words, the complement of the union of <em>all</em> \(A_i\) is the <em>intersection</em> of all of the complements of each \(A_i\).</p>
<p>\[ \left(\bigcap_{i=1}^n A_i\right)^c = \bigcup_{i=1}^n A_i^c \]
In other words, the complement of the intersection of <em>all</em> \(A_i\) is the <em>union</em> of the all of the complements of each \(A_i\).</p>
<h3 id="axioms-of-probability"><a class="header" href="#axioms-of-probability">Axioms of Probability</a></h3>
<p>We can define the probability of an event occurring by its relative frequency in \(n\) repetitions of an experiment as \(n \to \infty\).</p>
<p>Define \(P(A)\) as the probability that event \(A\) occurs and \(nA\) as the number of times in the first \(n\) repetitions of an experiment that the event \(A\) occurs.</p>
<p>\[P(A) = \lim_{n\to\infty}{\frac{nA}{n}} \]
\[P(A) = \frac{\text{number of elements in A}}{\text{number of possible outcomes}}\]</p>
<div class="def">
<p><strong>Nonnegativity</strong>: <em>\(P(A) \geq 0\) for every event \(A\).</em></p>
</div>
<div class="def">
<p><strong>Additivity</strong>: <em>if \(A\) and \(B\) are two disjoint events, then the probability of their union satisfies</em>
\[
P(A \cup B) = P(A) + P(B)
\]</p>
</div>
<p>This expression generalizes to
\[
P\left(\bigcup_{i=1}^\infty A_i\right) = \sum_{i=1}^{\infty} P\left(A_i\right)
\]</p>
<p>In other words, the probability of the union of all events in \(A\) is the sum of the probability of each event \(A_i\).</p>
<div class="def">
<p><strong>Normalization</strong>: <em>The probability of the entire sample space \(\Omega\) is equal to \(1\).</em></p>
<p>\[
P(\Omega) = 1
\]</p>
</div>
<h3 id="propositions"><a class="header" href="#propositions">Propositions</a></h3>
<ol>
<li>\(P(A^c)=1-P(A)\) for every event \(A\).</li>
<li>If \(A \subset B\), then \(P(A) \leq P(B)\).</li>
<li>\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</li>
</ol>
<p>*Note: proposition 3 can be generalized to any number of events.</p>
<h1 id="lecture-4"><a class="header" href="#lecture-4">Lecture 4</a></h1>
<h3 id="conditional-probability"><a class="header" href="#conditional-probability">Conditional Probability</a></h3>
<div class="def">
<p><strong>Conditional Probability</strong>: <em>The conditional probability that \(A\) has occurred given that \(B\) has occurred with \(P(B) \geq 0\) is defined as</em>
\[
P(A|B) = \frac{P(A\cap B)}{P(B)}
\]</p>
</div>
<p>Rather than thinking about \(P(A|B)\) within universe \(\Omega\), think of \(B\) as <em>replacing</em> \(\Omega\).</p>
<p>\(A\) can be thought of as an event that occurs within the universe \(B\).</p>
<p>If the possible outcomes are finitely many and equally likely, then
\[
P(A|B) = \frac{\text{number of elements in }A\cap B}{\text{number of elements in }B}
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> A bin contains 25 light bulbs, 5 of which are in good condition (last at least a month), 10 of which are partially defective (works initially for 2 days), and 10 which are totally defective. Given that a bulb is lit initially for 2 days, what is the probability that it will still be working after a week?
</summary>
<ul>
<li>
<p>Because the bulb works initially for 2 days, we know it's not totally defective. We can consider this \(D^c\), or the complement of the defective set. \(D^c=\frac{15}{25}\) because \(\frac{10}{25}\) are totally defective.</p>
</li>
<li>
<p>We're looking for the probability that a bulb is perfectly good (\(G\)) <em>given</em> that it's not totally defective. The intersection of the non-defective bulbs and the perfectly good bulbs is \(\frac{5}{25}\) - in other words, 5 of the remaining bulbs are in good condition.</p>
</li>
</ul>
<p>\[
P(G|D^c) = \frac{G\cap D^c}{P(D^c)} = \frac{\frac{5}{25}}{\frac{15}{25}}=\frac{1}{3}
\]</p>
</details>
</div>
<h3 id="extra-credit-answer-to-the-universe--42"><a class="header" href="#extra-credit-answer-to-the-universe--42">Extra Credit: Answer to the Universe = 42</a></h3>
<h3 id="the-law-of-total-probability"><a class="header" href="#the-law-of-total-probability">The Law of Total Probability</a></h3>
<p>Let \(A\) and \(B\) be events. We may express \(A\) as
\[
A = (A \cap B) \cup (A \cap B^c)
\]</p>
<p>We can also say that
\[
\begin{align}
P(A) &amp;= P(AB) + P(AB^c)\\
&amp;=P(A|B)P(B) + P(A|B^c)P(B^c)
\end{align}
\]</p>
<p>If the sample space \(\Omega\) can be partitioned into \(n\) mutually exclusive events \(A_1, A_2, \dots, A_n\), then
\[
\begin{align}
P(A) &amp;= P(A \cap B_1) + P(A \cap B_2) + \dots + P(A \cap B_n)\\
&amp;=P(A|B_1)P(B_1) + \dots + P(A|B_n)P(B_n)
\end{align}
\]</p>
<h3 id="bayes-formula"><a class="header" href="#bayes-formula">Bayes' Formula</a></h3>
<p>We can find the conditional probability \(P(A_i|B)\), in terms of \(P(B|A)\) using the law of total probability and Bayes' rule.</p>
<p>Let \(A_1, A_2, \dots, A_n\) be disjoint sets that form a partition of the sample space, and assume that \(P(A_i) &gt; 0\) for all \(i\). Then, for any event \(B | P(B) &gt; 0\), we have
\[
\begin{align}
P(A_i|B) &amp;= \frac{P(A_i)P(B|A_i)}{P(B)}\\
&amp;=\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1) + \dots + P(A_n)P(B|A_n)}
\end{align}
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> A bowl contains three apples and four oranges. Bob walks by and randomly selects a fruit. Steve walks by 10 minutes later and also randomly selects a fruit from the bowl. 
<p>Let \(B_a\) be the event that Bob choses an apple, \(B_o\) the event that Bob chooses an orange, \(S_a\) the event that Steve chooses an apple, and \(S_o\) the event that Steve chooses an orange.</p>
<p>What is the probability \(P(S_a)\) that Steve selects an apple? Use the law of total probability.</p>
</summary>
<p>\[
\begin{align}
P(B_a) &amp;= \tfrac{3}{7}\\
P(B_o) &amp;= \tfrac{4}{7}\\
P(S_a | B_a) &amp;= \tfrac{2}{6} = \tfrac{1}{3}\\
P(S_a | B_o) &amp;= \tfrac{3}{6} = \tfrac{1}{2}\\
P(S_a) &amp;= P(S_a | B_a)P(B_a) + P(S_a | B_o)P(B_o)\\
&amp;= \tfrac{1}{3}\cdot\tfrac{3}{7} + \tfrac{1}{2}\cdot\tfrac{4}{7} = \tfrac{3}{7}
\end{align}
\]</p>
<p>If Steve selected an orange, what's the possibility that Bob had also selected an orange?</p>
<p>\[
P(B_o | S_o) = \frac{}{} = \frac{\left(\tfrac{1}{2}\right)\left(\tfrac{4}{7}\right)}{\left(\tfrac{1}{2}\right)\left(\tfrac{4}{7}\right) + \left(\tfrac{2}{3}\right)\left(\tfrac{3}{7}\right)} = \frac{1}{2}
\]</p>
</details>
</div>
<div class="def">
<p><strong>Independent Events</strong>: <em>Events \(A\) and \(B\) are said to be independent if</em></p>
<p>\[
\begin{align}
P(A | B) &amp;= \tfrac{P(AB)}{P(B)} = P(A)\text{, or}\\
P(AB)&amp;=P(A)P(B)
\end{align}
\]</p>
</div>
<h1 id="lecture-5---random-variables"><a class="header" href="#lecture-5---random-variables">Lecture 5 - Random Variables</a></h1>
<h3 id="random-variables"><a class="header" href="#random-variables">Random Variables</a></h3>
<div class="def">
<p><em>A random variable is a real-valued function of the outcome of an experiment.</em></p>
</div>
<p>A function of a random variable defines another random variable.</p>
<p>We can associate with each random variable certain &quot;averages&quot; of interest, such as the mean and variance.</p>
<p>A random variable can be conditioned on an event or on another random variable.</p>
<p>There is a notion of independence of a random variable from an event or from another random variable.</p>
<h3 id="discrete-random-variables"><a class="header" href="#discrete-random-variables">Discrete Random Variables</a></h3>
<div class="def">
<p><em>A random variable is called discrete if its range (the set of values that it can take) is either finite or countably infinite.</em></p>
</div>
<p>A discrete random variable is a real-valued function of the outcome of an experiment that can take a finite or countably infinite number of values.</p>
<p>A discrete random variable has an associated probability mass function (PMF), which gives the probability of each numerical value that the random variable can take.</p>
<p>A function of a discrete random variable defines another discrete random variable, whose PMF can be obtained from the PMF of the original random variable.</p>
<h3 id="probability-mass-functions"><a class="header" href="#probability-mass-functions">Probability Mass Functions</a></h3>
<div class="def">
<p><em>For a discrete random variable \(X\), we denote the PMF of \(X\) as \(P_x\).</em></p>
</div>
<p>If \(x\) is any real number, the probability mass of \(x\), denoted \(p_x(x)\), is the probability of the event \(\{ X=x \}\) consisting of all outcomes that give rise to a value \(X\) equal to \(x\):
\[
p_x(x) = P(\{ X = x \})
\]</p>
<p>For each possible value \(x\) of \(X\),</p>
<ol>
<li>Collect all the possible outcomes that give rise to the event \(\{ X = x \}\).</li>
<li>Add their probabilities to obtain \(p_x(x)\).</li>
<li>Note that
\[
\sum_x{p_x(x)}=1
\]</li>
<li>For any set \(S\) of possible values of \(X\), we have
\[
P(X \in S) = \sum_{x \in S}{p_x(x)}
\]</li>
</ol>
<p>For example, consider two tosses of a coin and let \(X\) be the number of heads.
\[
p_x(x) = 
\begin{cases}
\frac{1}{4} \text{ if } x \in \{ 0, 2 \}\\
\frac{1}{2} \text{ if } x = 1\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>If \(X\) is the number of heads obtained on two independent tosses of a fair coin, the probability of at least one head is
\[
P(X &gt; 0) = \sum_{x=1}^{2}p_x(x) = \frac{1}{2}+\frac{1}{4}=\frac{3}{4}
\]</p>
<h3 id="the-bernoulli-random-variable"><a class="header" href="#the-bernoulli-random-variable">The Bernoulli Random Variable</a></h3>
<p>Suppose that a trial/experiment has an outcome that can be classified as either a success or a failure.</p>
<p>Let \(X\) be \(1\) if the outcome is a success and let \(X\) be \(0\) if the outcome is a failure. The PMF is:
\[
\begin{align}
p(0) &amp;= P[X=0]=1-p\\
p(1) &amp;= P[X=1]=p
\end{align}
\]</p>
<h3 id="the-binomial-random-variable"><a class="header" href="#the-binomial-random-variable">The Binomial Random Variable</a></h3>
<p>Repeat the independent Bernoulli trials \(n\) times.</p>
<p>\(X\) is said to be binomial with parameters \((n, p)\).</p>
<p>The Bernoulli RV can be considered binomial \((1, p)\).</p>
<p>We use the binomial distribution to find \(k\) successes in \(n\) independent trials.</p>
<p>\[
p(k \text{ successes in } n \text{ trials}) = \binom{n}{k}p^k\left(1-p\right)^{n-k}
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> Flip a fair coin 10 times. What is the probability that 6 heads appear?
</summary>
<p>\[
\begin{align}
p(6\text{ successes in } 10 \text{ trials}) &amp;= \binom{10}{6}\left(\tfrac{1}{2}\right)^6\left(\tfrac{1}{2}\right)^4\\
&amp;=\binom{10}{6}\left(\tfrac{1}{2}\right)^{10}
\end{align}
\]</p>
</details>
</div>
<h3 id="the-poisson-random-variable"><a class="header" href="#the-poisson-random-variable">The Poisson Random Variable</a></h3>
<p>Suppose the probability mass function of a random variable \(X\) is given by
\[
p_x(k) = \frac{c\lambda^k}{k!}, k = 0, 1, 2, \dots, \text{where } \lambda &gt; 0
\]</p>
<p>Knowing that all probabilities sum to 1, we must have
\[
c\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}=1
\]</p>
<p>Using the fact that the above property is the taylor series expansion of \(e^\lambda\), we know that \(c=e^{-\lambda}\).</p>
<p>Suppose we want to know \(P(X=0)\) and \(P(X&gt;2)\).
\[
\begin{align}
P(X=0) &amp;= e^{-\lambda}\\
P(X&gt;2) &amp;= 1-P(X=0)-P(X=1)-P(X=2)\\
&amp;=1-e^{-\lambda}-\lambda e^{-\lambda}-\frac{\lambda^2e^{-\lambda}}{2}
\end{align}
\]</p>
<p>We'll see that the Poisson RV is closely related to the Exponential RV. The parameter \(\lambda\) in the exponential RV is a rate (mean rate), whereas the parameter \(\lambda\) in the Poisson is typically a number (mean number). To alleviate the confusion, the Poisson RV parameter is frequently referred to as \(\alpha\).
\[
p_x(k) = e^{-\alpha}\frac{\alpha^k}{k!}, k = 0, 1, 2, \dots
\]</p>
<h3 id="the-geometric-random-variable"><a class="header" href="#the-geometric-random-variable">The Geometric Random Variable</a></h3>
<p>The number of Bernoulli trials \(k\) required for a success is geometrically distributed.
\[
p_x(k) = (1-p)^{k-1}p, k=1, 2, \dots
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong>
<p>If the probability of a station transmitting a packet successfully is \(\frac{1}{3}\), what is the probability that 2 transmissions are needed?</p>
</summary>
<p>\[
p_x(2) = \left(1-\tfrac{1}{3}\right)\left(\tfrac{1}{3}\right)
\]</p>
</details>
</div>
<h3 id="modified-geometric-random-variable"><a class="header" href="#modified-geometric-random-variable">Modified Geometric Random Variable</a></h3>
<p>Suppose that we wanted the number of failures <em>up until a success</em>.</p>
<p>This is the modified geometric RV.</p>
<p>\[
p_x(k) = (1-p)^kp, k=1, 2, \dots
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong>
<p>If the probability of a station transmitting a packet successfully is \(\tfrac{1}{3}\), what is the probability that there were 4 failures before a success?</p>
</summary>
<p>\[
\left(\tfrac{2}{3}\right)^4\left(\tfrac{1}{3}\right) = \tfrac{16}{243}
\]</p>
</details>
</div>
<h3 id="functions-of-random-variables"><a class="header" href="#functions-of-random-variables">Functions of Random Variables</a></h3>
<p>Given a random variable \(X\), one may generate other random variables by applying various transformations on \(X\).</p>
<p>Let the random variable \(X\) be the temperature in degrees Celsius and consider the transformation \(Y = 1.8X + 32\), which gives the temperature in degrees Fahrenheit.</p>
<p>In this example, \(Y\) is a linear function of \(X\) of the form
\[
Y = g(X) = aX + b
\]
where \(a\) and \(b\) are scalars.</p>
<p>We may also consider nonlinear functions of the general form \(Y = g(X)\).</p>
<p>To obtain \(p_Y(y)\) for any \(y\), we add the probabilities of all values of \(x\) such that \(g(x) = y\):
\[
p_Y(y) = \sum_{\{ x|g(x)=y \}}{p_X(x)}
\]</p>
<p>Let \(Y = |X|\) and let us apply the preceding formula for the PMF to the case where
\[
\begin{cases}
\frac{1}{9} \text{ if } x \in [-4, 4]\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>The possible values of \(Y\) are \(y=0, 1, 2, 3, 4\).</p>
<p>We have \(p_Y(0) = p_X(0) = \frac{1}{9}\).</p>
<p>Note that two values of \(X\) correspond to each \(y=1, 2, 3, 4\). For example:
\[
p_Y(1) = p_X(-1) + p_x(1) = \frac{2}{9}
\]</p>
<p>Thus, the PMF of Y is
\[
p_Y(y) = 
\begin{cases}
\frac{2}{9} \text{ if } y=1, 2, 3, 4\\
\frac{1}{9} \text{ if } y=0\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>For another related example, let \(Z = X^2\).</p>
<p>To obtain the PMF of \(Z\), we can view it either as the square of the random variable \(X\) or as the square of the random variable \(Y\).</p>
<p>If we let \(p_Z(z) = \sum{\{ y|y^2=z \}p_Y(y)}\), we obtain
\[
p_Z(z) =
\begin{cases}
\frac{2}{9} \text{ if } z=1, 4, 9, 16\\
\frac{1}{9} \text{ if } z=0\\
0 \text{ otherwise}
\end{cases}
\]</p>
<h3 id="expectation-mean-and-variance"><a class="header" href="#expectation-mean-and-variance">Expectation, Mean, and Variance</a></h3>
<p>We define the expected value (also called the <em>expectation</em> or the <em>mean</em>) of a random variable \(X\) with PMF \(p_X(x)\) by
\[
E[X] = \sum_{x}{xp_X(x)}
\]</p>
<p>Consider two independent coin tosses, each with a \(\tfrac{3}{4}\) probability of a head, and let \(X\) be the number of heads obtained. This is a binomial random variable with parameters \(n = 2\) and \(p = \tfrac{3}{4}\). Its PMF is
\[
p_X(k) =
\begin{cases}
\begin{align}
&amp;\left(\tfrac{1}{4}\right)^2 &amp;\text{if } k=0\\
&amp;2\cdot\left(\tfrac{1}{4}\right)\cdot\left(\tfrac{3}{4}\right) &amp;\text{if } k=1\\
&amp;\left(\tfrac{3}{4}\right)^2 &amp;\text{if } k=2\\
&amp;0 &amp;\text{otherwise}\\
\end{align}
\end{cases}
\]</p>
<p>So, the mean is</p>
<p>\[
\begin{align}
E[X] &amp;= 0 \cdot \left(\tfrac{1}{4}\right)^2 + 1 \cdot \left(2 \cdot \tfrac{1}{4} \cdot \tfrac{3}{4}\right) + 2 \cdot \left(\tfrac{3}{4}\right)^2\\
&amp;= \tfrac{24}{16}\\
&amp;= \tfrac{3}{2}
\end{align}
\]</p>
<h3 id="moments--variance"><a class="header" href="#moments--variance">Moments &amp; Variance</a></h3>
<p>The \(n^{th}\) moment of the random variable \(X\), denoted as \(E[X^n]\) is the expected value of the random variable \(X^n\).</p>
<p>The most important quantity associated with a random variable \(X\) other than the mean is its variance, which is denoted by \(\text{var}(X)\) and is defined as the expected value of the random variable \(\left(X-E[X]\right)^2\).</p>
<p>Note that this means the variance is <em>always</em> nonnegative.</p>
<p>THe variance provides a measure of dispersion of \(X\) around its mean. Another measure of dispersion is the standard deviation of \(X\), which is defined as the square root of the variance and is denoted by \(\sigma_X\).
\[
\sigma_X = \sqrt{\text{var}(X)}
\]</p>
<h3 id="expected-value-rule-for-random-variable-functions"><a class="header" href="#expected-value-rule-for-random-variable-functions">Expected Value Rule for Random Variable Functions</a></h3>
<p>Let \(X\) be a random variable with PMF \(p_X(x)\) and let \(g(X)\) be a real-valued function of \(X\). Then, the expected value of the random variable \(g(X)\) is given by
\[
E[g(X)] = \sum_{x}{g(x)p_X(x)}
\]</p>
<p>Let \(X\) be a random variable and let \(Y = aX+b\), where \(a\) and \(b\) are given scalars. Then, \(E[Y] = aE[X] + b\) and \(\text{var}(Y) = a^2\text{var}(X)\).</p>
<p>The variance of a random variable in terms of the moments expression is
\[
\text{var}(X) = E[X^2] - \left(E[X]\right)^2
\]</p>
<p>Again, consider two independent coin tosses, each with a \(\tfrac{3}{4}\) probability of a head. Let \(X\) be the number of heads obtained. This is a binomial random variable with parameters \(n=2\) and \(p=\tfrac{3}{4}\). Its PMF is
\[
p_X(k) =
\begin{cases}
\begin{align}
&amp;\left(\tfrac{1}{4}\right)^2 &amp;\text{if } k=0\\
&amp;2\cdot\left(\tfrac{1}{4}\right)\cdot\left(\tfrac{3}{4}\right) &amp;\text{if } k=1\\
&amp;\left(\tfrac{3}{4}\right)^2 &amp;\text{if } k=2\\
&amp;0 &amp;\text{otherwise}\\
\end{align}
\end{cases}
\]</p>
<p>The first two moments and variance are
\[
\begin{align}
E[X] &amp;= 0 \cdot \tfrac{1}{4}^2 + 1 \cdot \left(2 \cdot \tfrac{1}{4} \cdot \tfrac{3}{4}\right) + 2 \cdot \tfrac{3}{4}^2 = \tfrac{24}{16} = \tfrac{3}{2}\\
E[X] &amp;= 0 \cdot \tfrac{1}{4}^2 + 1^2 \cdot \left(2 \cdot \tfrac{1}{4} \cdot \tfrac{3}{4}\right) + 2^2 \cdot \tfrac{3}{4}^2 = \tfrac{24}{16} = \tfrac{21}{8}\\
\text{var}(X) &amp;= \tfrac{21}{8} - \tfrac{3}{2}^2 = \tfrac{3}{8}
\end{align}
\]</p>
<h3 id="discrete-uniform-random-variable"><a class="header" href="#discrete-uniform-random-variable">Discrete Uniform Random Variable</a></h3>
<p>What is the mean and variance of the roll of a fair six-sided die? Its PMF is
\[
p_X(k) =
\begin{cases}
\tfrac{1}{6} \text{ if } k=1, 2, 3, 4, 5, 6\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>\[
\begin{align}
E[X] &amp;= 3.5\\
\text{var}(X) &amp;= E[X^2]-\left(E[X]\right)^2\\
&amp;= \tfrac{1}{6}\left(1^2+2^2+3^2+4^2+5^2+6^2\right) - (3.5)^2\\
&amp;= \tfrac{35}{12}
\end{align}
\]</p>
<p>The above random variable is a special case of a <strong>discrete uniformly distributed random variable</strong>, or <strong>discrete uniform</strong> for short.</p>
<p>The discrete uniform random variable has a PMF of the form
\[
p_X(k) =
\begin{cases}
\tfrac{1}{b-a+1} \text{ if } k \in [a, b]\\
0 \text{ otherwise}
\end{cases}
\]
where \(a, b \in \mathbb{Z}, a &lt; b\).</p>
<p>The mean and variance are
\[
\begin{align}
E[X] &amp;= \frac{a+b}{2}\\
\text{var}(X) &amp;= E[X^2] - \left(E[X]\right)^2 = \frac{(b-a+1)^2-1}{12}
\end{align}
\]</p>
<h3 id="extra-credit-first-city-to-reach-1000000-people-rome"><a class="header" href="#extra-credit-first-city-to-reach-1000000-people-rome">Extra Credit: First city to reach 1,000,000 people: Rome</a></h3>
<h1 id="lecture-6"><a class="header" href="#lecture-6">Lecture 6</a></h1>
<h3 id="continuous-random-variables"><a class="header" href="#continuous-random-variables">Continuous Random Variables</a></h3>
<div class="def">
<p><strong>Continuous Random Variable</strong>: <em>A random variable \(X\) is called <strong>continuous</strong> if its porbability law can be described in terms of a nonnegative function \(f_X\), called the <strong>probability density function</strong> of \(X\), or <strong>PDF</strong> for short.</em></p>
</div>
<p>A PDF satisfies
\[
P(X \in B) = \int_B {f_X(x)}dx
\]</p>
<p>for every subset \(B\) of the real line. In particular, the probability that the value of \(X\) falls within an interval is
\[
P(a \leq X \leq b) = \int_a^b{f_X(x)}dx
\]</p>
<p>Note:
\[
P(a \leq X \leq b) = P(a &lt; X &lt; b) = P(a \leq X &lt; b) = P(a &lt; X \leq b)
\]</p>
<div class="idea">
<p><strong>Big Idea</strong>: <em>The probability at an individual point on a continuous distribution is 0, so the equality of \(a\) or \(b\) is unimportant.</em></p>
</div>
<p>Note that to qualify as a PDF, a function \(f_X\) must be nonnegative, i.e. \(f_X(x) \geq 0\) for every \(x\), and mus talso satisfy the normalization equation
\[
\int_{-\infty}^\infty{f_X(x)}dx = P(-\infty &lt; X &lt; \infty) = 1
\]</p>
<h3 id="continuous-uniform-random-variable"><a class="header" href="#continuous-uniform-random-variable">Continuous Uniform Random Variable</a></h3>
<p>Consider a random variable \(X\) that takes values in an interval \([a, b]\) and assume that all subintervals of the same length are equally likely. We refer to this type of random variable as uniform or uniformly distributed. Its PDF has the form
\[
f_X(x) =
\begin{cases}
c \text{ if } a \leq x \leq b\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>This means that we can calculate the value of c via the following:
\[
1 = \int_a^b{c}dx = c\int_a^b{dx} = c(b-a)
\]
In other words,
\[
c = \frac{1}{b-a}
\]</p>
<h3 id="piecewise-constant-pdf"><a class="header" href="#piecewise-constant-pdf">Piecewise Constant PDF</a></h3>
<div class="ex">
<details>
<summary>
<strong>Example:</strong> Alvin's driving time to work is between 15 and 20 minutes if the day is sunny, and between 20 and 25 minutes if the day is rainy, with all times being equally likely in each case.
<p>Assume that a day is sunny with probability \(\tfrac{2}{3}\) and rainy with probability \(\tfrac{1}{3}\).</p>
<p>What is the PDF of the driving time viewed as a random variable \(X\)?</p>
</summary>
<p>We interpret the statement that &quot;all times are equally likely&quot; in the sunny and rainy cases to mean that the PDF of \(X\) is constant in each of the intervals \([15, 20]\) and \([20, 25]\).</p>
<p>Furthermore, since these two intervals contain all possible driving times, the PDF should be zero everywhere else.</p>
<p>\[
f_X(x) =
\begin{cases}
c_1 \text{ if } 15 \leq x \leq 20\\
c_2 \text{ if } 20 \leq x \leq 25\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>where \(c_1, c_2\) are some constants.</p>
<p>We can determine these constants by using the probabilities of a sunny and of a rainy day.</p>
<p>\[
\begin{align}
\frac{2}{3} &amp;= P(\text{sunny day}) = \int_{15}^{20}{f_X(x)}dx = \int_{15}^{20}{c_1}dx = 5c_1\\
\frac{1}{3} &amp;= P(\text{rainy day}) = \int_{20}^{25}{f_X(x)}dx = \int_{20}^{25}{c_2}dx = 5c_2\\ 
\end{align}
\]</p>
<p>So that
\[
c_1 = \frac{2}{15}\quad c_2 = \frac{1}{15}
\]</p>
</details>
</div>
<h3 id="a-pdf-can-be-arbitrarily-large"><a class="header" href="#a-pdf-can-be-arbitrarily-large">A PDF can be arbitrarily large</a></h3>
<p>Consider a random variable \(X\) with PDF
\[
f_X(x) = 
\begin{cases}
\tfrac{1}{2\sqrt{x}} \text{ if } 0 &lt; x \leq 1\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>Even though \(f_X(x)\) becomes infinitely large as \(x\) approaches zero, this is still a valid PDF because
\[
\int_{-\infty}^\infty{f_X(x)}dx = \int_0^1{\frac{1}{2\sqrt{x}}}dx = \sqrt{x}\Big|_0^1 = 1
\]</p>
<h3 id="summary-of-pdf-properties"><a class="header" href="#summary-of-pdf-properties">Summary of PDF Properties</a></h3>
<p>Let \(X\) be a CRV with PDF \(f_X\).
\[
\begin{gather}
f_X(x) \geq 0 \text{ for all } x\\\\
\int_{-\infty}^\infty{f_X(x)}dx = 1\\\\
\text{If } \delta \text{ is very small, then } P(|x, x+\delta|) \approx f_X(x) \cdot \delta\\\\
\text{For any subset } B \text{ of the real line, } P(X \in B) = \int_B{f_X(x)}dx
\end{gather}
\]</p>
<h3 id="expectation"><a class="header" href="#expectation">Expectation</a></h3>
<p>The <strong>expected value</strong> or <strong>mean</strong> of a CRV \(X\) is defined by
\[
E[X] = \int_{-\infty}^\infty{xf_X(x)}dx
\]
We also have
\[
E[g(X)] = \int_{-\infty}^\infty{g(x)f_X(x)}dx
\]
The variance of \(X\) is defined by
\[
\text{var}(X) = E[(X - E[X])^2] = \int_{-\infty}^\infty{(X - E[X])^2f_X(x)}dx
\]
We have
\[
0 \leq \text{var}(X) = E[x^2] - (E[X])^2
\]
If \(Y=ax+b\) where \(a\) and \(b\) are given scalars, then
\[
E[Y] = aE[X] + b \quad \text{var}(Y) = a^2\text{var}(X)
\]</p>
<h3 id="cumulative-distribution-functions"><a class="header" href="#cumulative-distribution-functions">Cumulative Distribution Functions</a></h3>
<p>The CDF of a random variable \(X\) is denoted by \(F_X\) and provides the probability \(P(X \leq x)\). In particular, for every \(x\) we have</p>
<p>\[
F_X(x) = P(X \leq x) = 
\begin{cases}
\sum_{k \leq x}{p_X(k)} \text{ if } X \text{ discrete}\\\\
\int_{-\infty}^x{f_X(t)}dt \text{ if } X \text{ continuous}
\end{cases}
\]</p>
<h3 id="cdf-properties"><a class="header" href="#cdf-properties">CDF Properties</a></h3>
<p>The CDF \(F_X\) of a random variable \(X\) is defined by
\[
F_X(x) = P(X \leq x) \text{ for all x}
\]</p>
<ul>
<li>
<p>\(F_X\) is monotonically nondecreasing: if \(x \leq y\), then \(F_X(x) \leq F_X(y)\)</p>
</li>
<li>
<p>\(F_X(x)\) tends to 0 as \(x \to -\infty\), and to 1 as \(x \to \infty\).</p>
</li>
<li>
<p>If \(X\) is discrete, then \(F_X\) has a piecewise constant and staircase-like form.</p>
</li>
<li>
<p>If \(X\) is continuous, then \(F_X\) has a continuously increasing form.</p>
</li>
<li>
<p>If \(X\) is discrete and takes integer values, the PMF and the CDF can be obtained from each other by summing or differencing:
\[
\begin{gather}
F_X(k) = \sum_{t=-\infty}^{k}{p_X(i)}\\\\
p_X(k) = P(X \leq k) - P(X \leq k-1) = F_X(k) - F_X(k-1)
\end{gather}
\]
for all integers \(k\).</p>
</li>
</ul>
<h3 id="conditional-pdf-and-expectation"><a class="header" href="#conditional-pdf-and-expectation">Conditional PDF and Expectation</a></h3>
<p>The conditional PDF \(f_{X|A}\) of a continuous random variable \(X\) given an event \(A\) with \(P(A) &gt; 0\) satisfies
\[
P(X \in B|A) = \int_{B}{f_{X|A}(x)}dx
\]</p>
<p>If \(A\) is a subset of the real line with \(P(X \in A) &gt; 0\), then
\[
P(X \in B|X \in A) = \frac{P(X \in B \cap X \in A)}{P(X \in A)} = \frac{\int_{A \cap B}{f_X(x)}dx}{P(X \in A )}
\]</p>
<p>The resulting formula is
\[
f_{X|A}(x) = 
\begin{cases}
\frac{f_X(x)}{P(X \in A)} \text{ if } x \in A\\
0 \text{ otherwise}
\end{cases}
\]</p>
<p>and
\[
P(X \in B|X \in A) = \int_{B}{f_{X|A}(x)}dx
\]</p>
<p>for any set \(B\).</p>
<p>The corresponding conditional expectation is defined by
\[
E[X|A] = \int_{-\infty}^\infty{xf_{X|A}(x)}dx
\]</p>
<p>The expected value rule remains valid:
\[
E[g(X)|A] = \int_{-\infty}^\infty{g(x)f_{X|A}(x)}dx
\]</p>
<p>If \(A_1, A_2, \dots, A_n\) are disjoint events with \(P(A_i) &gt; 0\) for each \(i\) that form a partition of the sample space, then
\[
f_X(x) = \sum_{i=1}^{n}P(A_i)f_X|A(x)
\]</p>
<p>Law of total expectation:
\[
\begin{align}
E[X] &amp;= \sum_{i=1}^{n}{P(A_i)E[X|A_i]}\\
E[g(X)] &amp;= \sum_{i=1}^{n}{P(A_i)E[g(X)|A_i]}
\end{align}
\]</p>
<h3 id="multiple-continuous-random-variables"><a class="header" href="#multiple-continuous-random-variables">Multiple Continuous Random Variables</a></h3>
<p>Two continuous random variables associated with a common experiment are jointly continuous and can be described in terms of a joint PDF \(f_{X,Y}\) if \(f_{X,Y}\) is a nonnegative function that satisfies
\[
P((X, Y) \in B) = \int_{(x, y) \in B}{\int{f_{X,Y}(x, y)}dx}dy
\]</p>
<p>For every subset \(B\) of the two-dimensional plane.</p>
<p>If \(B\) is a rectangle of the form \(B = [a, b] \times [c, d]\), we have
\[
P(a \leq X \leq b, c \leq Y \leq d) = \int_{c}^{d}{\int_{a}^{b}{f_{X,Y}(x, y)}dx}dy
\]</p>
<p>We have the normalization property
\[
\int_{-\infty}^\infty{\int_{-\infty}^\infty{f_{X,Y}(x, y)}dx}dy = 1
\]</p>
<h3 id="marginal-distributions"><a class="header" href="#marginal-distributions">Marginal Distributions</a></h3>
<p>Given the joint random variable \(f_{x, y}(x, y)\), the marginals are:
\[
\begin{align}
f_X(x) = \int_{-\infty}^\infty{f_{X,Y}(x, y)}dy\\
f_Y(y) = \int_{-\infty}^\infty{f_{X,Y}(x, y)}dx\\
\end{align}
\]</p>
<p>Given the joint random variables of three variables \(f_{X,Y,Z}(x, y, z)\), the marginals are:
\[
\begin{align}
f_X(x) = \int_{-\infty}^\infty{\int_{-\infty}^\infty{f_{X,Y,Z}(x,y,z)}dy}dz\\
f_Y(y) = \int_{-\infty}^\infty{\int_{-\infty}^\infty{f_{X,Y,Z}(x,y,z)}dx}dz\\
f_Z(z) = \int_{-\infty}^\infty{\int_{-\infty}^\infty{f_{X,Y,Z}(x,y,z)}dx}dy\\
\end{align}
\]</p>
<h3 id="expectation-1"><a class="header" href="#expectation-1">Expectation</a></h3>
<p>If \(X\) and \(Y\) are jointly continuous random variables, and \(g\) is some function, then \(Z = g(X, Y\) is also a random variable.
\[
E[g(X, Y)] = \int_{-\infty}^\infty{\int_{-\infty}^\infty{g(x,y)f_{X,Y}(x,y)}dx}dy
\]</p>
<p>As an important special case, for any scalars \(a, b\), we have
\[
E[aX + bY] = aE[X] + bE[Y]
\]</p>
<h3 id="conditioning"><a class="header" href="#conditioning">Conditioning</a></h3>
<p>Let \(X\) and \(Y\) be continuous random variables with joint PDF \(f_{X,Y}\). For any fixed \(y\) with \(f_Y(y) &gt; 0\), the conditional PDF of \(X\) given that \(Y=y\) is defined by
\[
f_{X|Y}(x|y) = \frac{f_{X|Y}(x|y)}{f_Y(y)}
\]</p>
<p>The variable \(y\) is usually a fixed number and \(f_{X|Y}(x|y)\) is a function of the single variable \(x\).</p>
<p>Note:
\[
\int_{-\infty}^\infty{f_{X|Y}(x|y)}dx = 1
\]</p>
<p>So for any fixed \(y\), \(f_{X|Y}(x|y)\) is a legitimate PDF.</p>
<p>Conditional Expectation:
\[
\begin{align}
E[X|Y = y] &amp;= \int_{-\infty}^\infty{xf_{X|Y}(x|y)}dx\\
E[g(X)|Y=y] &amp;= \int_{-\infty}^\infty{g(x)f_{X|Y}(x|y)}dx
\end{align}
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong>
<p>Let \(X\) be exponentially distributed with mean 1. Once we observe the experimental value \(x\) of \(X\), we generate a normal random variable \(Y\) with zero mean and variance \(x+1\). What is the joint PDF of \(X\) and \(Y\)?</p>
</summary>
We have:
<p>\[
f_X(x) = e^{-x} \text{ for } x \geq 0 \text{ and } f_{Y|X}(y|x) = \frac{1}{\sqrt{2\pi(x+1)}}e^\frac{{-y^2}}{2(x+1)}
\]</p>
<p>The joint distribution is
\[
f_{X,Y}(x,y) = f_X(x)f_{Y|X}(y|x) = e^{-x}\frac{1}{\sqrt{2\pi(x+1)}}e^\frac{{-y^2}}{2(x+1)}
\]
for all \(x \geq 0\) and all \(y\).</p>
</details>
</div>
<h3 id="discrete-uniform-random-variable-1"><a class="header" href="#discrete-uniform-random-variable-1">Discrete Uniform Random Variable</a></h3>
<p>In many situations, we have a model of an underlying but unobserved phenomenon represented by a random variable \(X\) with PDF \(f_X\), and we make noisy measurements \(Y\) which is \(f_{Y|X}\).</p>
<p>Once the experimental value of \(Y\) is measured, what information does this provide on the unknown value of \(X\)?</p>
<p>Recall that when we used Bayes' rule for discrete distributions, we can go in both directions.</p>
<p>\[
f_Xf_{Y|X} = f_{X,Y} = f_Yf_{X|Y}
\]</p>
<p>We have the expression
\[
f_Xf_{Y|X} = \frac{f_X(x)f_{Y|X}(y|x)}{\int_{-\infty}^\infty{f_X(t)f_{Y|X}(y|t)}dt}
\]</p>
<div class="ex">
<details>
<summary>
<strong>Example:</strong>
<p>A lightbulb is known to have an exponentially distributed lifetime \(Y\). However, the company has been experiencing quality control problems. On any given day, the parameter \(\lambda\) of the PDF of \(Y\) is actually a random variable, uniformly distributed in the interval \([1, \tfrac{3}{2}]\). We test a lightbulb and record the experimental value \(y\) of its lifetime. What can we say about the underlying parameter \(\lambda\)?</p>
</summary>
<p>We model the parameter \(\lambda\) as a uniform random variable \(\Lambda\) with PDF:
\[
f_\Lambda(\lambda) = 2 \text{ for } 1 \leq \lambda \leq \tfrac{3}{2}
\]
All available information about \(\Lambda\) is contained int he conditional PDF \(f_{\Lambda|Y}(\lambda|y)\), which is given by:</p>
<p>\[
f_{\Lambda|Y}(\lambda|y) = \frac{2\lambda e^{-\lambda y}}{\int_{1}^{\frac{3}{2}}{2te^{-ty}}}dt \text{ for } 1 \leq \lambda \leq \frac{3}{2}
\]</p>
</details>
</div>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
